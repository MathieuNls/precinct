
\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{xcolor}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{listings}
\usepackage[linesnumbered]{algorithm2e}
\usepackage{tikz}

\definecolor{bblue}{HTML}{4F81BD}
\definecolor{rred}{HTML}{C0504D}
\definecolor{ggreen}{HTML}{9BBB59}
\definecolor{ppurple}{HTML}{9F4C7C}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}

\title{PRECINCT:\@ An Incremental Approach for Preventing Clone Insertion at Commit Time}


\author{\IEEEauthorblockN{Mathieu Nayrolles, }
\IEEEauthorblockA{Software Behaviour Analysis (SBA) Research Lab\\
ECE, Concordia University\\
Montreal, Canada\\
m\_nayrol@ece.concordia.ca}
\and
\IEEEauthorblockN{Abdelwahab Hamou-Lhadj}
\IEEEauthorblockA{Software Behaviour Analysis (SBA) Research Lab\\
ECE, Concordia University\\
Montreal, Canada\\
abdelw@ece.concordia.ca}}

% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
Software clones are considered harmful since they may cause the same buggy code to appear in multiple places in the code, making software maintenance and evolution tasks challenging. Clone detection has been an active research field for almost two decades.
Interestingly, most existing techniques focus on detecting clones after they are inserted in the code.
In this paper, we take another look at the clone detection problem by designing a novel approach for preventing the insertion of clones in the first place. Our approach, called PRECINCT (PREventing Clones INsertion at Commit Time),  detects efficiently  near-miss software clones at commit time by means of pre-commit hooks.
This way, changes to the code are analyzed and suspicious copies are flagged before they reach the central code repository in the version control system.  The application of PRECINCT to three systems developed independently shows that PRECINCT would have prevented 92.4\% of clone insertion.



\end{abstract}


\IEEEpeerreviewmaketitle

\section{Introduction}
\label{sec:Introduction}


Code clones appear when developers reuse code with little to no modification to the original code.
Studies have shown  that clones can account for about 7\% to 50\% of code in a given software system\cite{Baker, StephaneDucasse}.
Developers often reuse code (and create clones) in their software on purpose\cite{Kim2005}.
Nevertheless, clones are considered a bad practice in software development since they can introduce new bugs in the code\cite{Kapser2006,Juergens2009,Li2006}.
If a  bug is discovered in one segment of the code that has been copied and pasted several times, then the developers will have to remember the places where this segment has been reused in order to fix the bug in each of them.

In the last two decades, there have been many studies and tools that aim at detecting clones. They can be grouped into three categories.
The first category includes techniques that treat the source code as text and use transformation and normalization  to compare various code fragments\cite{Johnson1994,Johnson1993, Cordy2011, Roy2008}.
The second category includes methods that
use lexical analysis, where the source code is sliced into sequences of tokens, similar to the way a compiler operates\cite{Baker,Bakera,Baker2002,Kamiya2002,Li2006}.
The tokens are used to compare code fragments.
Finally, syntactic analysis has also been performed where the source code is converted into trees, more particularly abstract syntax tree (AST), and then the clone detection is performed using tree matching algorithms\cite{Baxter1998, Komondoor2000, Tairas2006, Falke2008}.

Despite the advances in clone detection research, the use of exiting clone detection tools is not as widespread as one might think.
The main factors that contribute to this are summarized in \cite{Johnson2013}. These tools are known to output a large number of data, making it hard to understand and analyse their results. In addition, they tend to have a high amount of false positives. Finally, they are hard to configure and do not integrate well with the day-to-day workflow of a developers.

In this paper, we present PRECINCT (PREventing Clones INsertion at Commit Time) that focuses on preventing the insertion of clones in the first place at commit time, i.e., before they reach the central code reposiroy. PRECINCT uses pre-commit hooks capabilities of modern source code version control systems. A pre-commit hook is a process that one can implement to receive the latest modification to the source code done by a given developer just before the code reaches the central repository.
PRECINCT intercepts this modification and analyses its  content to see whether a suspicious clone has been introduced or not.
A flag is raised if a code fragment is suspected to be a clone of an exiting code segment.
In fact, PRECINCT, itself, can be seen as a pre-commit hook that detects clones that might have been inserted in the latest changes with regard to the rest of the source code.
This said, only a fraction of the code is analysed, making PRECINCT efficient compared to leading  clone detection techniques such as NICAD (Accurate Detection of Near-miss Intentional Clones) \cite{Cordy2011}.
Moreover, the detected clones are presented using a classical 'diff' output that developers are familiar with.  PRECINCTS is easy to use and configure. It only takes  one command line to configure PRECINCT.
PRECINCT is also well integrated with the workflow of the developers since it is used in conjunction with a source code version control systems such as Git and Jira.

[WAHAB: YOU NEED TO ADD A PARAGRAPH TALKING ABOUT CLONE TYPES AND MAKING THE CASE FOR TYPE 3-1, WHILE EXPLAINING THAT PRECINCT FOCUSES ON TYPE 3-1]

We evaluated the effectiveness of PRECINCT using precision and recall on three systems, developed independently and written in both C and Java. The results show that PRECINCT prevents near-miss software clones to reach the source version system with an average accuracy of 92.4\% accuracy, while increasing the size of the repository by only 3\% to 5\%. 

The rest of this paper is organized as follows: In Section~\ref{sec:Related Work}, we present the studies related to PRECINCT, with a particular emphasis on NICAD \cite{Cordy2011}, used to build PRECINCT. Then, in Section~\ref{sec:The PRECINCT Approach}, we present the PRECINCT approach. The evaluation of PRECINCT is the subject of  Section~\ref{sec:Experimentations}.
Finally, we propose concluding remarks in Section~\ref{sec:Conclusion}.

\section{Related Work}
\label{sec:Related Work}

Clone detection is not an important (but a difficult) task. Throughout the years, researchers and practitioners have developed a considerable number of methods and tools in order to detect efficiently source code clones.

Text-based techniques use the code --- often raw (e.g. with comments) --- and compare sequences of code (blocks) to each other in order to identify potential clones. Johnson was perhaps the first one to use fingerprints to detect clones\cite{Johnson1993,Johnson1994}. Blocks of code are hashed; producing fingerprints that can be compared.
If two blocks share the same fingerprint, they are considered as clones.
Manber et al. \cite{Manber1994} and Ducasse et al.\cite{Ducasse1999} refined the fingerprint technique by using leading keywords and dot-plots, respectively.

Tree-matching and metrics are two sub-categories of syntactic analysis for clone detection. Syntactic analysis consists of building abstract syntax trees (AST) and analyses them with a set of dedicated metrics or searching for identical sub-trees. Many approaches using AST have been published using sub-trees comparison including the work of Baxter et al.\cite{Baxter1998}, Wahleret al. \cite{Wahler}, or more recently, the work of Jian et al. with Deckard \cite{Jiang2007}.
AST-based approach compare metrics computed on the AST, rather than the code itself, to identify clones \cite{Patenaude1999, Balazinska}.

Another approach to detect clones is to use static analysis and to leverage the semantics of the program to improve the detection.
These techniques rely on program dependency graphs where nodes are statements and edges are dependencies.
Then, the problem of finding clones is reduced to the problem of finding identical sub-groups in the program dependency graph.
Examples of recent techniques that fall into this category are the ones presented by Krinke et al.\cite{Krinke2001} and  Gabel et al. \cite{Gabel2008}.

Many clone detection tools have been created using a lexical approach for clone detection. Here, the code is transformed into a series of tokens. If sub-series repeat themselves, it means that a potential clone is in the code. Some popular tools that use this technique include, but not limited to, Dup\cite{Baker}, CCFinder\cite{Kamiya2002}, and CP-Miner\cite{Li2006}.

Furthermore, a large number of taxonomies have been published in an attempt to classify  clones and hence directs  research on clone detection\cite{Mayrand1996,Balazinska1999,Koschke2006,Bellon2007,NeilDavey,Kontogiannis,Kapser}.

Other active research activities in clone detection focus on clone removal and management. Once detected, an obvious step is to provide approaches to remove clones in an automatic way or (at least) keep track of them if removing them is not an option.
Most modern IDEs provide the \textit{extract method} feature that transforms a potentially copy-based block of code into a method and a call to the newly generated method\cite{Komondoor,higo2004refactoring}.
More advanced techniques involve analysing the output of CCFinder\cite{Bomarius2004} or program dependencies graphs\cite{higo2004refactoring} to automatically suggest a method that would go through the \textit{extract method} process
Codelink\cite{Toomim} and\cite{Duala-Ekoko2007}.

The aforementioned  techniques, however, focus on detecting clones after they are inserted in the clonde. A few studies only focus on preventing the insertion of clones in the first place. Lague et al. \cite{Lague} conducted a very large empirical study with 10,000 [WHAT] over 3 years, where developers where asked to use clone detection tools during the development process of a very large Telecom system. The authors found that while clones are being removed over time, using clone detection tools help improving the quality of the system as it prevent defects to reach the customers. Duala et al. \cite{Duala-Ekoko2007,Duala-Ekoko2010} proposed to create clone region descriptors (CRDs), which describe clone regions within methods in a robust way that is independent from the exact text of the clone region or its location in a file. Then, using CRDs, clone insertion can be prevented.

PRECINCT aims to prevent clone insertion while integrating the clone detection process in a transparent manner in the day-to-day development process. This way, software developers do not have to resort to external tools to remove clones after they are inserted. Our approach operates at commit time, notifying software developers of possible clones as they commit their code. 


\section{The PRECINCT Approach}
\label{sec:The PRECINCT Approach}

\begin{figure*}
  \centering
    \includegraphics[width=\textwidth]{media/approach.png}
    \caption{ Overview of the PRECINCT Approach.\label{fig:precinct-approach}}
\end{figure*}

The PRECINCT approach is composed of six steps.
The first step is the commit step where developers send their latest change to the central repository and the last step is the reception of the commit by the central repository.
The second step is the pre-commit hook which kicks in as the first operation when one wants to commit.
The pre-commit hook has access to the changes in terms of files that have been modified, more specifically, the lines that have been modified. The modified lines of the files are sent to TXL\cite{Cordy2006a} for block extraction. Then, the blocks are compared to previously extracted blocks in order to identify candidate clones  using NICAD\cite{Cordy2011}.
 Finally, the output of NICAD is further refined and presented to the user for a decision round. These steps are discussed in more detail in the following subsections.

\subsection{Commit}
\label{sub:Commit}

In version control systems, a commit adds the latest changes made to the source code to the repository, making these changes part of the head revision of the repository.
Commits in version control systems are kept in the repository indefinitely. Thus, when other users do an update or a checkout from the repository, they will receive the latest committed version, unless they wish to retrieve a previous version of the source code in the repository.
Version control systems allow rolling back to previous versions easily.
In this context, a commit within a version control system is protected as it is easily rolled back, even after the commit has been done.

\subsection{Pre-Commit Hook}
\label{sub:Pre-Commit Hook}

Hooks are custom scripts set to fire off when certain important actions occur.
There are two groups of  hooks: client-side and server-side.
Client-side hooks are triggered by operations such as committing and merging, whereas server-side hooks run on network operations such as receiving pushed commits.
These hooks can be used for all sorts of reasons (GIVE ONE OR TWO EXAMPLES).


The pre-commit hook is run first, before one even types in a commit message. It is used to inspect the snapshot that is about to be committed.
This is used to allow developers to check if they have not forgotten anything, to run tests, or to examine whatever the need to inspect the code.
% Exiting [WAHAB: I DO NOT UNDERSTAND THIS SENTENCE] non-zero from this hook aborts the commit, although one can bypass it with a no-verify commit (a feature that exists in Git and other version systems).
Depending on the exit status of the hook, the commit will be aborted and not pushed to the central repository.
Also, developers can choose to ignore the pre-hook by using \texttt{git commit --no-verify} instead of \texttt{git commit}.
This can be useful in case of emergency bug fix where the code has to reach the central repository as quick as possible.
Developers can do things like check for code style (run lint or something equivalent), check for trailing white spaces (the default hook does exactly this), or check for appropriate documentation on new methods.

PRECINCT is a set of bash scripts where the entry point of these scripts lies in the pre-commit hooks. Pre-commit hooks are easy to create and implement as depicted in Listing~\ref{gitprehook}.
The pre-hook is shipped in version control systems like Git\footnote{https://git-scm.com/}.
Note that even though we use Git as the main version control to present PRECINCT, we believe that the techniques presented in this paper are readily applicable to other version control systems.
From lines 3 to 11, the script identifies if the commit is the first one in order to select the revision to work against.
Then, in Lines 18 and 19, the script checks for trailing whitespace and fails if any are found.

\noindent\begin{minipage}{0.90\linewidth}

  \lstinputlisting[language=Bash, firstnumber=1, numbers=right, stepnumber=1,leftmargin=30, label=gitprehook, caption=Git Pre-Commit Hook Sample]{media/pre-commit.sample}

\end{minipage}

For PRECINCT to work, we just have to add the call to our script suite instead or in addition of the whitespace check.

\subsection{Extract and Save Blocks}
\label{sub:Extract and Save Blocks}

A block is a set of consecutive lines of code that will be compared to all other blocks in order to identify clones.
To achieve this critical part of PRECINCT, we rely on TXL\cite{Cordy2006a}, which is a first-order functional programming over linear term rewriting, developed by Cordy et al.\cite{Cordy2006a}.
For TXL to work, one has to write a grammar describing the syntax of the source  language and the transformations needed. TXL has three main phases: \textit{parse, transform}, \textit{unparse}.
In the parse phase, the grammar controls not only the input but also the output form.
Listing~\ref{txlsample} --- extracted from the official documentation\footnote{http://txl.ca} --- shows a grammar matching a \textit{if-then-else} statement in C with some special keywords: [IN] (indent), [EX] (exdent) and [NL] (newline) that will be used for the output form.

\noindent\begin{minipage}{0.90\linewidth}

  \lstinputlisting[language=Bash, firstnumber=1, numbers=right, stepnumber=1,leftmargin=30, label=txlsample, caption=Txl Sample Sample]{media/txl.sample}

\end{minipage}

Then, the \textit{transform} phase will, as the name suggests, apply transformation rules that can, for example, normalize or abstract the source code. Finally, the third phase of TXL,  called \textit{unparse}, unparses the transformed parsed input in order to output it.
Also, TXL supports what the creators call Agile Parsing\cite{Dean}, which allow developers to redefine rules of the grammar and, therefore, apply different rules than the original ones.


PRECINCT takes advantage of that by redefining the blocks that should be extracted for the purpose of clone comparison, leaving out the  blocks that are out of scope.
More precisely, before each commit, we only extract the blocks belonging to the modified parts of the source code, hence reducing the size of the output, a common issue of clone detection techniques as discussed in the introductory section.

We have selected TXL for several reasons. First, TXL is easy to install and to integrate with a developer workflow.
 Second, it was relatively easy to create a grammar that accepts commits as input.
 This is because TXL is shipped with C, Java, Csharp, Python and WSDL grammars that define all the particularities of these languages, with the ability to customize these grammar to to accept changesets (chunks of the modified source code that includes the added, modified, and deleted lines) instead of the whole code.


Algorithm~\ref{alg:extract} presents an overview of the "extract" and "save" blocks operations.

\begin{algorithm}[H]
 \KwData{$Changeset[]$ changesets\;
 $Block[]$ prior\_blocks\;
 $Boolean$ compare\_history\;
 }
 \KwResult{Up to date blocks of the systems}
 \For{$i \leftarrow 0$ \KwTo$size\_of~changesets$}{
    Block[] blocks $\leftarrow$ $extract\_blocks(changesets)$\;
    \For{$j \leftarrow 0$ \KwTo$size\_of~blocks$}{
       \If{not $compare\_history$ AND $blocks[j]$ overrides one of $prior\_blocks$}{
          delete $prior\_block$\;
       }
       write $blocks[j]$\;
    }
 }

 \SetKwProg{myproc}{Function}{ $~extract\_blocks(Changeset~cs)$}{}
   \myproc{\proc{}}{

   \uIf{$cs~is~unbalanced~right$}{$cs \leftarrow expand\_left(cs)$\;}

   \ElseIf{$cs~is~unbalanced~left$}{$cs \leftarrow expand\_right(cs)$\;}

   \nl\KwRet$txl\_extract\_blocks(cs)$\;
   }

 \caption{Overview of the Extract Blocks Operation\label{alg:extract}}
\end{algorithm}

This algorithm receives as arguments, the changesets, the blocks that have been previously extracted and a boolean named compare\_history.
Then, from Lines 1 to 9 lie the $for$ loop that iterates over the changesets. For each changeset (Line 2), we extract the blocks by calling the $~extract\_blocks(Changeset~cs)$ function.
In this function, we expand our changeset to the left and to the right in order to have a complete block.
As depicted by Listing~\ref{commitsample}, changesets contain only the modified chunk of code and not necessarily complete blocks. Indeed, we have a block from Line 3 to Line 6 and deleted lines from Line 8 to 14.
However, in Line 7 we can see the end of a block but we do not have its beginning. Therefore, we need to expand the changeset to the left in order to have syntactically correct blocks.
We do so by checking block's beginning and ending, \{ and \} in C for example.
Then, we send these expanded changesets to TXL for block extraction and formalization.

\noindent\begin{minipage}{0.90\linewidth}

  \lstinputlisting[language=Bash, firstnumber=1, numbers=right, stepnumber=1,leftmargin=30, label=commitsample, caption=Changeset c4016c of monit]{media/commit.sample}

\end{minipage}


For each extracted block, we check if the current block overrides (replaces) a previous block (Line 4).
In such a case, we delete the previous block as it does not represent the current version of the program anymore (Line 5).
Also, we have an optional step in PRECINCT defined in Line 4. The compare\_history is a condition to delete overridden blocks.

% [WAHAB: I DON'T UNDERSTAND THE FOLLOWING 3 SENTENCES. PLEASE REPHRASE]
We believe that deleted blocks have been deleted for a good reason (bug, default, removed features, \ldots) and if a newly inserted block matches an old one, it could be worth knowing in order to improve the quality of the system at hand.
This feature is deactivated by default.

In summary, this step receives the files and lines, modified by the latest changes made by the developer and produces an up to date block representation of the system at hand. The blocks can be analysed in the next step to discover potential clones.

\subsection{Compare Extracted Blocks}
\label{sub:Compare Extracted Blocks}

In order to compare the extracted blocks and detect potential clones we can only resort to text-based techniques.
This is because lexical and syntactic analysis approaches (alternatives to text-based compariosn) would require a complete program to work, a program that compiles. In the relatively wide-range of tool and techniques that exist to detect clones by considering code as text\cite{Johnson1993,Johnson1994,Marcus,Manber1994,StephaneDucasse,Wettel2005}, we select NICAD as the main text-based method for comparing clones \cite{Cordy2011} for several reasons.
First, NICAD is built on top of TXL, which we also used TXL in the previous step.
Second, NICAD is able to detect all types of clones.

NICAD  works in three phases: \textit{Extraction}, \textit{Comparison} and \textit{Reporting}. During the \textit{Extraction} phase all potential clones are identified, pretty-printing, and extracted.
We do not use the \textit{Extraction} phase of NICAD as it has been built to work on programs that are syntactically correct, which is not the case of changesets.
We replaced NICAD's \textit{Extraction} phase by our own tools, described in the previous section.

In the \textit{Comparison} phase, extracted blocks are transformed, clustered and compared in order to find potential clones.
Using TXL sub-programs, blocks go through a process called pretty-printing where they are stripped of formatting and comments.
When code fragments are cloned, some comments, indentation or spacing are changed according to the new context where the new code is used. This pretty-printing process ensures that all code will have the same spacing and formatting and ease the comparison.
Furthermore, in the pretty-printing process, statements can be broken down into several lines.
Table~\ref{tab:pretty-printing} shows how this can improve the accuracy of clone detection with three \texttt{for} statements, \texttt{ for (i=0; i<10; i++)}, \texttt{for (i=1; i<10; i++)} and \texttt{ for (j=2; j<100; j++)}.
The pretty-printing allows NICAD to detect Segments 1 and 2 as a clone pair because only the initialization of $i$ changed.
This specific example would not have been marked as a clone by other  tools we tested such as Duploc\cite{Ducasse1999}.
In addition to the pretty-printing, code can be normalized and filtered to detect different classes of clones and match user preferences.

\input{pretty-printing-table.tex}

Finally, the extracted, pretty-printed, normalized and filtered blocks are marked as potential clones using a Longest Common Subsequence (LCS) algorithm\cite{Hunt1977}. Then, a percentage of unique statements can be computed and, depending on a given threshold (see Section~\ref{sec:Experimentations}), the blocks are marked as clones.

The last step of NICAD, which acts as our clone comparison engine, is the \textit{reporting}. However, to prevent PRECINCT from outputting  a large number of data (an issue from which many clone detection techniques face), we  implemented our own reporting system, which is also well embedded with wthe workflow of developers. This reporting system is the subject of the next section.

As a summary, this step receives potentially expanded and balanced blocks from the extraction step.
Then, the blocks are pretty-printed, normalized, filtered and fed to an LCS algorithm in order to detect potential clones.

\subsection{Output and Decision}
\label{sub:Output and Decision}

In this final step, we report the result of the clone detection at commit time with respect to the latest changes made by the developer. The process is straightforward. Every change made by the developers has been through the previous steps and might have been marked as a potential clone. For each file that is suspected to contain a clone, one line is printed to the command line with the following options: (I) Inspect, (D) Disregard, (R) Remove from the commit as shown by Figure X.

(I) Inspect will cause a diff-like visualization of the suspected clones (Figure Y) while (D) disregard will simply ignore the finding.
To integrate PRECINCT in the workflow of the developer we also propose the  remove option (R). This option will simply remove the suspected file from the commit that is about to be sent to the central repository.
Also, if the user types an option key twice, e.g. II, DD or RR, then, the option will be applied to all files.
For instance, if the developer types DD at any one point, the results output by PRECINCT will be disregarded and the commit will be allowed to go through. We believe that this simple mechanism will encourage developers to use PRECINCT like they would use any other feature of Git (or any other control version system).



\section{Experimentations}
\label{sec:Experimentations}

In this section, we show the effectiveness of PRECINCT to
detect clones at commit time in three open source systems\footnote{The programs used and instructions to reproduce the experiments are made available for download from https://research.mathieu-nayrolles.com/precinct/}.

The aim of the case study is to answer the following question: \textit{Can we detect insertion clones at commit time, i.e., before they are inserted in the final code?}

\subsection{Target Systems}
\label{sub:Target Systems}

Table~\ref{tab:sut} shows the systems used in this study and their characteristics in terms of the number files they contain and the size in KLOC(Kilo Lines of Code). We also include the version and the programming language in which the system is written.

\begin{table}[]
\centering
\caption{List of Target Systems in Terms of Files and Kilo Line of Code (KLOC) at current version and Language}
\label{tab:sut}
\resizebox{0.5\textwidth}{!}{%
\begin{tabular}{c|c|c|c|c}
SUT        & Revisions & Files & KLoC & Language \\ \hline\hline
Monit      & 826       & 264   & 107  & C        \\ \hline
Jhotdraw   & 735       & 1984  & 44   & Java     \\ \hline
dnsjava    & 1637      & 233   & 47   & Java     \\ \hline
\end{tabular}
}
\end{table}

Monit [REF or FOOTNOTE] is a small open source utility for managing and monitoring Unix systems.
Monit is used to conduct automatic maintenance and repair and supports the ability to identify causal actions to detect errors.
This system is written in C and composed of 826 revisions, 264 files, and the latest version has 107 KLoC.
We have chosen Monit as a target system because it was one of the systems NICAD was tested on.

JHotDraw [REF or FOOTNOTE] is a Java GUI framework for technical and structured Graphics.
It has been developed as a ''design exercise''. Its design relies heavily on the use of design patterns. JHotDraw is composed of 735 revisions, 1984 files, and the latest revision has 44 KLoC.It is written and Java and it is often used by researchers as a test bench. JHotDraw was also used by NICAD's developers to evaluate their approach.

Dnsjava \cite{Wellington2013} is a tool for implementing of the DNS (Domain Name Service) mechanisms in Java.
This tool can be used for queries, zone transfers, and dynamic updates.
It is not as large as the other two, but it still makes an interesting case subject because it has been well maintained for the past decade. Also, this tool is used in many other popular tools such as Aspirin, Muffin and
Scarab. Dnsjava is composed of 1637 revisions, 233 files and 47 KLoC at the latest revision.
We have chosen this system because we are familiar with it as we used it before\cite{Nayrolles2015c}. This can help us analyse the results easier.

\subsection{Process}
\label{sub:Process}


Figure \ref{fig:precinct-branching} shows the process we followed to validate the effectiveness of PRECINCT.

\begin{figure}
  \centering
    \includegraphics[width=0.3\textwidth]{media/branch.png}
    \caption{PRECINCT Branching.\label{fig:precinct-branching}}
\end{figure}

As our approach relies on commit pre-hooks to detect possible clones during the development process (more particularly at commit time), we had to find a way to \textit{replay} past commits. To do so, we  \textit{cloned} our test subjects, and then created a new branch called \textit{PRECINCT\_EXT}.
When created, this branch is reinitialized at the initial state of the project (the first commit) and each commit can be replayed as they have originally been. At each commit, we store the time taken for PRECINCT and NICAD to run as well as the number of detected clone pairs detected.
The clone pairs that are detected are near-miss exact clones (Type 3-1) with a maximum line difference of 30\% as discussed in~\ref{tab:pretty-printing}.
As discussed in the introductory section, we chose to focus on Type 3-1 clones because (1) they are often introduced  intentionally \cite{Kim2005} and (2) they tend to be are hard to detect.

\input{tables-result}

\subsection{Results}
\label{sub:Results}

Figures~\ref{fig:r1},~\ref{fig:r2},~\ref{fig:r3} show the results of our study in terms of clone pairs that are detected per revision for our three subject systems: Monit, JHotDraw and Dnsjava. We compared our approach to NICAD that detects the clones after they are inserted. The blue line shows the clone detection performed by NICAD, while the red line shows the clone pairs that have been detected by PRECINCT (i.e., before they reach the central source repository).

[WAHAB: YOU NEED ALSO A TABLE THAT SUMMARIZES THE RESULTS. ALSO, YOU NEED TO LABEL THE Y-AXIS of ALL THE GRAPHS]

The first version of Monit contains 85 clone pairs and this number stays stable until Revision 100. From Revision 100 to 472 the detected clone pairs vary between 68 and 88 before reaching 219 at Revision 473. The number of clone pairs goes down to 122 at Revision 491 and decreases to 128 in the last revision. PRECINCT was able to detect 96.1\% of the clone pairs detected by NICAD (123/128) with 100\% recall. It took around 1 second for PRECINCT to execute on  a standard XXXX [WAHAB: describe the machine you are using]. 

JHotDraw starts with 196 clone pairs at Revision 1 and reaches a pick of 2048 at Revision 180. The number of clones  continues to go up until Revisions 685 and 686 where the number of pairs is 1229 before picking at 6538 and more from Revisions 687 to 721.
PRECINCT was able to detect 98.3\% of the clone pairs detected by NICAD (6490/6599) with 100\% recall while executing on average in 1.7 second.

For dnsjava, the number of clone pairs starts high with 258 clones and goes up until Revision 70 where it reaches 165. Another quick drop is observed at Revision 239 where we found only 25 clone pairs. The number of clone pairs stays stable until Revision 1030 where it reaches 273. PRECINCT was able to detect 82.8\% of the clone pairs detected by NICAD (226/273) with 100\% recall while executing on average in 1.1 second.

Overall, PRECINCT would have prevented in average 92.4\% of the clone to reach the central source code repository while executing more than twice as fast as NICAD (1.2 sec versus 3.8 sec). The difference in execution time between NICAD and PRECINCT stems from the fact that, unlike PRICINCT, NICAD is not an incremental approach. For each revision, NICAD has to extract all the code blocks and then compares all the pairs with each other ($n^2$). On the other hand, PRECINCT only extracts blocks when they are modified and only compares what has been modified to the rest of the program.

The difference of precision between NICAD and PRECINCT (7.3\%)  can be explained by the fact that sometimes developers commit code that does not compile. Such commits will still count as a revision, but TXL fails to extract blocks that do not comply with the target language syntax. While NICAD also fails in such a case, the disadvantage of PRECINCT comes from the fact that the failed block is saved and used as reference until it is changed by a correct one in another commit.

\section{Threats to Validity}
\label{sec:Threats to Validity}

The selection of SUTs is one of the common threats to validity for approaches aiming to improve the understanding of program's behavior.
It is possible that the selected programs share common properties that we are not aware of and therefore, invalidate our results.
However, the SUTs analyzed by PRECINCT are the same as the ones used in similar studies.
Moreover, the SUTs vary in terms of purpose, size and history.

Another threat to validity lies in the way we have selected the versions used in this study.
We selected version randomly to avoid any bias.
One may argue that a better approach would be to select revision based on size or diversity.
However, we believe that our approach do not depends on the characteristics of the revision but on the characteristics of the SUT.

In addition, we see a threat to validity that stems from the fact that we only used open source systems. The results may not be generalizable to industrial systems. We intend to undertake these studies in future work.

The programs we used in this study are all based on the Java, c and python programming languages. This can limit the generalization of the results. However, similar to Java, c, python and wsdl if one writes a TXL grammar --- which can be a relatively hard work --- in order to make PRECINCT available for a new language.

Finally, we use NICAD as clones detection engine but we believe that PRECINCT is generalizable for any clone detection engine.

In conclusion, internal and external validity have both been minimized by choosing a relatively large set of different systems and using input data that can be found in other programming languages.



\section{Conclusion}
\label{sec:Conclusion}







\bibliographystyle{IEEEtran}
\bibliography{library.bib}


\end{document}
